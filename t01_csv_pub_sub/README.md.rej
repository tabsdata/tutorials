diff a/t01_csv_pub_sub/README.md b/t01_csv_pub_sub/README.md	(rejected hunks)
@@ -1,18 +1,22 @@
-In this tutorial, we’ll explore how Tabsdata enables Pub/Sub for Tables.
 
-We'll start by setting up the system and creating a publisher that reads data from a CSV file called `customers.csv` stored in an input directory in the local file system and selects certain columns of interest from it. This data will be published as a table called `CUSTOMER_LEADS` within a collection called `CUSTOMERS`. Collections are containers for related tables in Tabsdata, to make data organization and management more efficient. 
+# Tutorial 1: Pre-processing, Publishing and Subscribing a CSV (`t01_csv_pub_sub`)
+
 
-Next, we'll configure a subscriber to read data from this table and write it to an output directory on the local file system. 
+In this tutorial, we’ll explore how Tabsdata enables Pub/Sub for Tables.
 
-Finally, we'll implement automated data engineering using Tabsdata to streamline the propagation of changes in the input files to downstream users.
+We will start by setting up the Tabsdata server and registering a publisher that reads data from a CSV file, selects
+some aspects of it, and publishes it as a table within the system. Following that, we will register a subscriber that
+subscribes to this published table, and exports it to the file system in a JOSN lines format. We will then demonstrate
+that when the publisher is rerun to load new data, the subscriber automatically receives it in their external system.
 
-In a real-world scenario, your data source could be a database, an S3 bucket, or another storage location, while the subscriber could write data to various endpoints such as a database or file system.
+In a real-world scenario, your data source could be a database, an S3 bucket, or another storage location, while the
+subscriber could write data to various endpoints such as a database or file system.
 
-Let’s dive in! We’ll start by setting up the system to prepare us to work with the Tabsdata functions.
+Let’s dive in!
 
-# Step 1. Setting up the system
+## Step 1 - Setting up the system
 
-## 1. Install Tabsdata
+### 1.1 Install Tabsdata
 
 To install/update the Tabsdata Python package, run this command in your CLI:
 
@@ -197,39 +344,72 @@ Here is some sample data from `customer_leads.jsonl`:
 {"IDENTIFIER":"37-41/89","GENDER":"Male","NATIONALITY":"Cuban","LANGUAGE":"Luxembourgish","OCCUPATION":"Telex Operator"}
 ```
 
-As you see from the output file, only the columns selected from the `customers.csv` defined in `publisher.py` file have been exported, and the `jsonl` file is ready for consumption.
+As you see from the output file, only the columns selected from the `customers.csv` defined in `publisher.py` file have
+been exported, and the `jsonl` file is ready for consumption.
 
-# Step 4: Automate Data Engineering
+## Step 4: Automatic execution of dependencies
 
 What happens when there is an update in your input data? How do you update the data used by the downstream users?
 
-Let’s say there is an update in your CSV file, and 20 new customers get added to the CSV file. The `customers.csv` file in the `input_02` folder presents one such scenario. The file has 20 new customers in addition to the customers present in the `customers.csv` file in the `input` folder.
+Let’s say there is an update in your CSV file, and 20 new customers get added to the CSV file. The `customers_02.csv`
+file in the `input` directory presents one such scenario. This file has 20 new customers in addition to the customers
+present in the `customers_01.csv` file that we loaded via the publisher when we triggered it for the first time.
 
 Here are details of 3 of these new customers from the 20 who have been added:
 
 | IDENTIFIER | NAME       | SURNAME | FIRST_NAME | LAST_NAME | FULL_NAME       | GENDER | GENDER_CODE | GENDER_SYMBOL | SEX    | PHONE_NUMBER  | TELEPHONE      | EMAIL                          | BIRTHDATE  | NATIONALITY | LANGUAGE | LOCALE    | BLOOD_TYPE | HEIGHT | WEIGHT | UNIVERSITY                                         | ACADEMIC_DEGREE | TITLE | OCCUPATION           | POLITICAL_VIEWS | WORLDVIEW          | USERNAME          | PASSWORD     |
 |-------------|------------|---------|-------------|-----------|------------------|--------|--------------|----------------|--------|----------------|-----------------|-------------------------------|------------|--------------|-----------|------------|-------------|--------|--------|-----------------------------------------------------|------------------|-------|----------------------|------------------|---------------------|--------------------|--------------|
 | 21-12/62     | Dakota      | Baxter  | Louetta      | Myers     | Tracy Ball        | Other  | 1            | ♂               | Female | +12272974320   | +16146882188    | drainage2086@duck.com         | 2022-11-04 | Swiss        | Zulu      | Locale.EN  | A+          | 1.79   | 38     | Western Connecticut State University (WCSU)        | PhD               | Mr.   | Medical Technician     | Moderate          | Pantheism           | networks_1867       | Gvwp+R+N     |
-| 97-89/11     | Christinia  | Espinoza| Elden        | Alvarado  | Gerald Wolfe      | Female | 0            | ⚲               | Other  | +1-402-266-2114| +1-479-878-9781 | livestock1811@example.org     | 2014-05-09 | Dominican    | Yiddish   | Locale.EN  | A+          | 1.80   | 74     | Florida Gulf Coast University (FGCU)               | PhD               | Ms.   | Maid                  | Socialist          | Atheism             | throw_1882          | }&<h*EYp     |
+| 97-89/11     | Christinia  | Espinoza| Elden        | Alvarado  | Gerald Wolfe      | Female | 0            | ⚲               | Other  | +1-402-266-2114| +1-479-878-9781 | livestock1811@example.org     | 2014-05-09 | Dominican    | Yiddish   | Locale.EN  | A+          | 1.80   | 74     | Florida Gulf Coast University (FGCU)               | PhD               | Ms.   | Maid                  | Socialist          | Atheism             | throw_1882          | ?     |
 | 92-54/93     | Perry       | Herman  | Amina        | Montgomery| Lory Justice      | Other  | 0            | ♀               | Female | +1-817-696-6699| +1-213-091-1513 | dynamic2052@duck.com          | 2018-12-24 | Salvadorian  | Greek     | Locale.EN  | B+          | 1.66   | 42     | University of South Florida (USF)                   | Bachelor           | B.Sc  | Town Planner           | Libertarian        | Secular humanism    | excitement_1908      | ]X4&n9yn     |
 
 <br/>
 
-**Important:** To simulate the new customers data being available as input, you need to replace the `customers.csv` file in the `input` folder with the `customers_new.csv` file in `input_02` and rename the it to `customers.csv`. With this, the new `customers.csv` in the `input` folder would have the data of 20 new customers.
+### 4.1 Provisioning new input file
+
+Before we can demonstrate the automatic execution of this workflow, we must provision the new input file in the correct
+location for the publisher to read and publish it accordingly. This can be done using the following command:
+
+```
+cp $TDX/input/customers_02.csv $TDX/input/customers.csv
+```
+
+This will overwrite the `customers.csv` file that was previously copied from `customers_01.csv` file for our first
+execution.
 
-Once the new input file is available, you just need to execute the publisher `publish_customers` using the command below to update the data files used by the downstream users.
+### 4.2 Saving first output file for comparison
+
+When this new workflow executes, the subscriber will overwrite the output file `$TDX/output/customer_leads.jsonl` with
+new data. To ensure this, let's create a backup of this file for later comparison using the following command:
+
+```
+cp $TDX/output/customer_leads.jsonl $TDX/output/customer_leads_01.jsonl
+```
+
+### 4.3 Trigger the pub/sub workflow
+
+The publisher function that we registered earlier creates a table called `CUSTOMER_LEADS`. This table in turn has a
+registered subscriber. Together, this publisher/subscriber pair makes a simple data engineering workflow. When the
+publisher activates and updates the table, it will automatically trigger any subscribers for the updated tables.
+
+To demonstrate this, we will trigger our publisher function manually. This should automatically trigger the subscriber
+function which in turn should overwrite our expected output file. Since the new input file has 20 more customer
+records, we expect that the output file will also have 20 more customer records available.
+
+Use the following command to trigger the publisher to read new input file:
 
 ```
 td fn trigger --collection CUSTOMERS --name publish_customers
 ```
 
-Once the publisher executes, all the downstream functions that are dependent on the output table `CUSTOMER_LEADS` from `publish_customers` would get executed to generate their respective output data.
+Even though there is only one subscriber that was executed on refresh of the published table, it will work for any
+number of subscribers that are registered.
 
-In our current example, since `susbcribe_customers` is dependent on the `CUSTOMER_LEADS` table, the subscriber would get executed to generate a new version of `customer_leads.jsonl` file in the local file system. You can read more about the automated dependency management in the Tabsdata documentation.
 
-## Check the Output:
+### 4.4 Check the subscriber output
 
-Once the publisher has been executed, you can check the `customer_leads.jsonl` file in the `output` folder to see if the changes are getting reflected.
+Once the publisher has been executed, you can check the `customer_leads.jsonl` file in the `output` folder to see if
+the changes are getting reflected.
 
 Here is some sample data from the new `customer_leads.jsonl`:
 
