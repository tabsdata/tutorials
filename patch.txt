diff --git a/.gitignore b/.gitignore
index 8b77f12..bee8a64 100644
--- a/.gitignore
+++ b/.gitignore
@@ -1 +1 @@
-archive/
\ No newline at end of file
+__pycache__
diff --git a/README.md b/README.md
index eebc93b..473d24d 100644
--- a/README.md
+++ b/README.md
@@ -1,11 +1,15 @@
 # README
 
-## Tutorial 1: Pre-processing, Publishing and Subscribing a CSV
+## Tutorial 1: Pre-processing, Publishing and Subscribing a CSV (`t01_csv_pub_sub`)
 
 In this first tutorial, we’ll explore how Tabsdata enables Pub/Sub for Tables.
 
-We'll start by setting up the system and creating a publisher that reads data from a CSV file called `customers.csv` stored in an input directory in the local file system, and selects certain columns of interest from it. This data will be published as a table called `CUSTOMER_LEADS` within a collection called `CUSTOMERS` in the Tabsdata system. 
+We'll start by setting up the system and creating a publisher that reads data from a CSV file called `customers.csv`
+stored in an input directory in the local file system, and selects certain columns of interest from it. This data will
+be published as a table called `CUSTOMER_LEADS` within a collection called `CUSTOMERS` in the Tabsdata system.
 
-Next, we'll configure a subscriber to read data from this table and write it to an output directory on the local file system.
+Next, we'll configure a subscriber to read data from this table and write it to an output directory on the local file
+system.
 
-Finally, we'll implement automated data engineering using Tabsdata to streamline the propagation of changes in the input files to downstream users.
\ No newline at end of file
+Finally, we'll implement automated data engineering using Tabsdata to streamline the propagation of changes in the
+input files to downstream users.
diff --git a/t01_csv_pub_sub/.gitignore b/t01_csv_pub_sub/.gitignore
new file mode 100644
index 0000000..d844230
--- /dev/null
+++ b/t01_csv_pub_sub/.gitignore
@@ -0,0 +1,2 @@
+customers.csv
+output
diff --git a/t01_csv_pub_sub/README.md b/t01_csv_pub_sub/README.md
index 7404163..360b6b2 100644
--- a/t01_csv_pub_sub/README.md
+++ b/t01_csv_pub_sub/README.md
@@ -1,18 +1,22 @@
-In this tutorial, we’ll explore how Tabsdata enables Pub/Sub for Tables.
 
-We'll start by setting up the system and creating a publisher that reads data from a CSV file called `customers.csv` stored in an input directory in the local file system and selects certain columns of interest from it. This data will be published as a table called `CUSTOMER_LEADS` within a collection called `CUSTOMERS`. Collections are containers for related tables in Tabsdata, to make data organization and management more efficient. 
+# Tutorial 1: Pre-processing, Publishing and Subscribing a CSV (`t01_csv_pub_sub`)
+
 
-Next, we'll configure a subscriber to read data from this table and write it to an output directory on the local file system. 
+In this tutorial, we’ll explore how Tabsdata enables Pub/Sub for Tables.
 
-Finally, we'll implement automated data engineering using Tabsdata to streamline the propagation of changes in the input files to downstream users.
+We will start by setting up the Tabsdata server and registering a publisher that reads data from a CSV file, selects
+some aspects of it, and publishes it as a table within the system. Following that, we will register a subscriber that
+subscribes to this published table, and exports it to the file system in a JOSN lines format. We will then demonstrate
+that when the publisher is rerun to load new data, the subscriber automatically receives it in their external system.
 
-In a real-world scenario, your data source could be a database, an S3 bucket, or another storage location, while the subscriber could write data to various endpoints such as a database or file system.
+In a real-world scenario, your data source could be a database, an S3 bucket, or another storage location, while the
+subscriber could write data to various endpoints such as a database or file system.
 
-Let’s dive in! We’ll start by setting up the system to prepare us to work with the Tabsdata functions.
+Let’s dive in!
 
-# Step 1. Setting up the system
+## Step 1 - Setting up the system
 
-## 1. Install Tabsdata
+### 1.1 Install Tabsdata
 
 To install/update the Tabsdata Python package, run this command in your CLI:
 
@@ -20,9 +24,10 @@ To install/update the Tabsdata Python package, run this command in your CLI:
 pip install tabsdata --upgrade
 ```
 
-Please note that you need **Python 3.12 or later**, to install the package. Additionally, you need **Tabsdata python package 0.9.2 or later** to successfully run the functions from this article.
+Please note that you need **Python 3.12 or later**, to install the package. Additionally, you need **Tabsdata python
+package 0.9.2 or later** to successfully run the functions from this article.
 
-## 2. Start the Tabsdata Server
+### 1.2 Start the Tabsdata Server
 
 To start the Tabsdata server, use the following command:
 
@@ -36,7 +41,7 @@ To verify that the Tabsdata server instance is running:
 tdserver status
 ```
 
-## 3. Copy the github repo
+### 1.3 Copy the github repo
 
 If you haven't already, copy the github repo to your system.
 
@@ -44,11 +49,14 @@ If you haven't already, copy the github repo to your system.
 git clone https://github.com/tabsdata/tutorials
 ```
 
-## 4. Save the working directory path in an environment variable
+### 1.4 Setting up directory path for referencing files
 
-As shared earlier, our data input in this example is present in a directory in the local file system, and the data output is sent to another directory in the local file system. To define these input and output directories we will use an environment variable called `TDX`. You can choose to define the absolute working directory of your input and output files in any other manner of your choice.
+In this tutorial, our data source is a CSV file on our file system in a particular input directory. Similarly, our
+table subscriber will use capture the table data in a JSON line format file in a particular output directory.
 
-To store the value of your working directory in the variable `TDX`, run the following command in your CLI from the directory where you have copied the Github repo:
+For convenience we will use an environment variable called `TDX` for referencing the input and output location used
+by the publisher and subscriber functions. Before we can do that, let's setup this variable to point to the base
+directory of this tutorial. You can do this using the appropriate commands from below:
 
 For Linux or MacOS:
 
@@ -71,9 +79,8 @@ If you run an `ls` on `t01_csv_pub_sub` you would see the following files and fo
 ```
 README.md
 input/
-|__ customers.csv
-input_02/
-|__ customers_new.csv
+|__ customers_01.csv
+|__ customers_02.csv
 publisher.py
 subscriber.py
 assets/
@@ -81,9 +88,16 @@ assets/
 |_table_schema.png
 ```
 
-Here the folders `input` and `input_02` contain the `customers.csv` and `custoemrs_new.csv` files respectively that serve as input files for our tutorial. We'll start by using the file in the `input` folder. Towards the end of the tutorial, we'll use the file in `input_02`. Python source files - `publisher.py` and `subscriber.py` - contain the publisher and subscriber functions. Feel free to take a peak at them - they are pretty straightforward. The assets folder is to hold some image assets for the README file.
+Here the folder `input` contains two files `customers_01.csv` and `customers_02.csv` that serve as input files. In
+order to use these files, we will copy them over to `input/customers.csv` which will be used by the publisher function.
+Later we will replace this file by `customers_02.cvs` by overwriting it to demonstrate that the newly published
+data is automatically delivered to the subscriber.
 
-## 5. Login to the Tabsdata server
+There are two Python source files - `publisher.py` and `subscriber.py` - which contain the publisher and subscriber
+functions. Feel free to take a peak at them - they are pretty straightforward. The assets folder has images for this
+README file.
+
+### 1.5 Login to the Tabsdata server
 
 Before you can use Tabsdata, you must login to the server which can be done as follows:
 
@@ -92,30 +106,54 @@ td login localhost --user admin --password tabsdata
 ```
 
 
-# Step 2: Publishing the CSV to Tabsdata
+## Step 2: Publishing the input CSV as a table
+
+
+Now that Tabsdata server is up and running, we can proceed to create our first publisher. A publisher is a simple
+Python function that uses built-in connectors provided by Tabsdata to read data from external source(s) and map
+it to one or more tables. A few things of note before we proceed:
+
+* The publisher function uses decorators to define the input data source details and output table names.
+* A publisher function is registered to a _Collection_ which acts as a container for tables. Consequently, any table(s)
+created by the registered publisher function are contained in the collection where it is registered.
+* The actual act of reading data from the data source(s) and publishing it to defined table(s) only happens when the
+publisher function is invoked by a _trigger_. In this example, we will manually trigger the publisher function to make
+it read the CSV file and publish it to a table within Tabsdata.
+
+
+### 2.1 Creating a collection
+
+In order to register our first publisher, we must create a collection. By default there are no collections within
+Tabsdata unless you create one. You can see this by running the following command:
 
-Once set up, we can now use Tabsdata to process the CSV file to select certain columns from it and publish the resultant data as a table.
+```
+td collection list
+```
 
-Tabsdata organizes data using collections, which act as containers for structured datasets. Hence, before publishing the data, we first create a `CUSTOMERS` collection to store the table to be created inside Tabsdata. To do that, run the following command in your CLI:
+For this tutorial, we will create a collection called CUSTOMERS where we will register our publisher function. To
+create this collection use the following command:
 
 ```
 td collection create CUSTOMERS
 ```
 
-Once the `CUSTOMERS` collection has been created, you can publish the CSV as a Tabsdata table. To do that, run the following CLI commands from your working directory:
+This should have created the collection that you can verify by running the previous list command. You can also see
+more details about this collection using the `info` command as follows:
 
 ```
-td fn register --collection CUSTOMERS --fn-path publisher.py::publish_customers
-td fn trigger --collection CUSTOMERS --name publish_customers
+td collection info CUSTOMERS
 ```
 
-In the above commands you are first registering the function `publish_customers` with the Tabsdata collection and then triggering its execution. You can read more about these steps in the [Tabsdata documentation](https://docs.tabsdata.com/latest/guide/04_working_with_functions/main_1.html).
+### 2.2 Registering the publisher function
+
+We will now register a publisher function that reads data from a CSV file on a specific input directory and publishes
+some selected columns of this data to a table. For convenience, we have this function ready to use in the file
+`publisher.py` and the name of this function is `publish_customers`. Here is what this function looks like:
 
-The Tabsdata function `publish_customers` reads the `customers.csv` file from the local file system, selects certain columns from it, and writes the data as a table in the `CUSTOMERS` collection as a table called `CUSTOMER_LEADS`. The function is defined in the `publisher.py` file in the working directory, with the following Python code:
 
 ```
 @td.publisher(
-    source = td.LocalFileSource(os.path.join(os.getenv("TDX"), "input",  "customers.csv")),
+    source = td.LocalFileSource(os.path.join(os.getenv("TDX"), "input", "customers.csv")),
     tables = ["CUSTOMER_LEADS"],
 )
 
@@ -124,15 +162,64 @@ def publish_customers(tf: td.TableFrame):
     return tf
 
 ```
-where,
 
-**source** defines the location of the file.
+Here the `@td.publisher` decorator defines the following metadata:
+* Data will be read from a a local file located at `$TDX/input/customers.csv`
+* And the output of this function will be publised as a table called `CUSTOMER_LEADS`
+
+The function definition is very simple in this case with the following details:
+* The function name is `publish_customers` that takes a `TableFrame` called `tf`. Note that a `TableFrame` is the API
+similar to a traditional `DataFrame` for use with Tabsdata. Note also that when this function executes, this input
+`TableFrame` will be populated by the data read from the `$TDX/input/customers.csv` file as specified in the decorator.
+* This function selects five specific columns from the input `TableFrame` and returns it as an output. Note that this
+output `TableFrame` will be mapped to a table called `CUSTOMER_LEADS` as specified in the decorator.
+
+That is all there is to a publisher. In a real world scenario, your publisher function can have many more inputs and
+may produce many more out outputs. Moreover, the body of the function may do more complex operations on the data before
+publishing them to output tables.
+
+Register this publisher function to the `CUSTOMERS` collection using the following command.
+
+```
+td fn register --collection CUSTOMERS --fn-path $TDX/publisher.py::publish_customers
+```
+
+You can now verify that the function was registered successfully by running the following command:
+
+```
+td fn list --collection CUSTOMERS
+```
+
+This should confirm that the function `publish_customers` has been registered within the collection `CUSTOMERS`.
+
+
+### 2.3 Triggering the publisher for the first time
+
+As a reminder, registering a function in a collection does not execute it, and it must be invoked by a trigger. And if
+a publisher function has never been triggerd, its corresponding output tables will not be initialized in the system.
+
+Before we manually trigger the publisher function, we must make sure that the input CSV file exists in the correct
+path. For our first run we will copy the provided sample input file `customers_01.csv` to the input location using
+the following command:
+
+```
+cp $TDX/input/customers_01.csv $TDX/input/customers.csv
+```
+
+With this input CSV file now in place, let's trigger our publisher. This can be done using the following command:
+
+```
+td fn trigger --collection CUSTOMERS --name publish_customers
+```
+
+Once a function is triggerd, a detailed execution plan is created and set in motion by Tabsdata. You can read more
+about this in [Tabsdata documentation](https://docs.tabsdata.com/latest/guide/04_working_with_functions/main_1.html).
 
-**tables** defines the name of the table (`CUSTOMER_LEADS`) to be created by the publisher.
 
-## Check the Publisher Output:
+### 2.4 Checking the publisher output
 
-The Tabsdata table `CUSTOMER_LEADS` has been created in the `CUSTOMERS` collection. This table can now be subscribed to, by various stakeholders within the organization.
+The Tabsdata table `CUSTOMER_LEADS` has been created in the `CUSTOMERS` collection. This table can now be subscribed
+to, by various stakeholders within the organization.
 
 To check the schema of the table in Tabsdata, run this command in your CLI:
 
@@ -144,7 +231,8 @@ Output:
 
 <img src="./assets/table_schema.png" alt="Schema" width="300">
 
-The columns `$td.id` and `$td.src` are internal columns created by Tabsdata to track row level lineage and provenance of data.
+The columns `$td.id` and `$td.src` are internal columns created by Tabsdata to track row level provenance
+of data.
 
 To check the sample of the table in Tabsdata, run this command in your CLI:
 
@@ -156,38 +244,97 @@ Output:
 
 <img src="./assets/table_sample.png" alt="Sample" height="300">
 
-# Step 3: Subscribing to the Table in Tabsdata
+## Step 3: Subscribing to a published Table in Tabsdata
 
-Now that the customer data is available in the Tabsdata system as a table, it’s ready for subscription. For instance, if the sales team requests customer data in JSON list format for their SaaS software, you can simply point them to the `CUSTOMER_LEADS` table in Tabsdata. They can then subscribe to this table to receive the data. To simulate this process and subscribe to the `CUSTOMER_LEADS` table, run the following CLI commands from your working directory:
+With the customer data available in Tabsdata as a table, it’s now ready for subscription. To demonstrate this we will
+create our first subscriber. A subscriber is a simple Python function that reads data from tables published within
+Tabsdata and uses built-in connectors provided by Tabasdata to send the data out to an external system. A few things
+of note before we proceed:
+
+* The subscriber function uses decorators to define the input table names an output data destinations.
+* A subscriber function is registered to a Collection just like a publisher function. For the purposes of this tutorial
+we will register our subscriber function within the same collection that we previously created -- `CUSTOMERS`.
+* The actual act of reading data from the input tables and writing it to external systems only happens when the
+subscriber function is invoked by a trigger. In this tutorial we will demonstrate two types of triggers for a
+subscriber function -- a manual trigger that we will do to create our first output; and a dependency trigger that
+automatically that happens when the source table changes.
 
-```
-td fn register --collection CUSTOMERS --fn-path subscriber.py::subscribe_customers
-td fn trigger --collection CUSTOMERS --name subscribe_customers
-```
 
-In the above commands you are first registering the function `subscribe_customers` with the Tabsdata collection and then triggering its execution. You can read more about these steps in the [Tabsdata documentation](https://docs.tabsdata.com/latest/guide/04_working_with_functions/main_1.html).
+### 3.1 Registering the subscriber function
 
-The Tabsdata function `subscribe_customers` reads the `CUSTOMER_LEADS` table from Tabsdata, and writes it as `customer_leads.jsonl` in the local file system. It is defined in the `subscriber.py` file in the working directory, with the following Python code:
+We will now register a subscriber function that reads data from the CUSTOMERS_LEADS table created by our publisher
+function in the prior steps, and externalizes this data in JSON line format to a specific output directory. For
+convenience we have this function ready to use in the `subscriber.py` and the name of the function is
+`subscribe_customers`. Here is what this function looks like:
 
 ```
 @td.subscriber(
     tables = ["CUSTOMER_LEADS"],
-    destination = td.LocalFileDestination(os.path.join(os.getenv("TDX"), "output","customer_leads.jsonl")), 
+    destination = td.LocalFileDestination(os.path.join(os.getenv("TDX"), "output", "customer_leads.jsonl")),
 )
 
 def subscribe_customers(tf: td.TableFrame):
     return tf
 ```
 
-where,
+Here the `@td.subscriber` decorator defines the following metadata:
+* Input data will be read from the table called `CUSTOMER_LEADS`
+* And output of this function will be pushed to a file located at `$TDX/output/customer_leads.jsonl`
+
+The function definition is very simple with following details:
+* The function name is `subscribe_customers` that takes a `TableFrame` as input. When executed, this input will be
+populated by the data coming from the `CUSTOMER_LEADS` table.
+* The function simply returns the input data as its output, which is mapped to a specific output file as defined
+by the decorator.
+
+That is all there is to a subscriber. In a real world scenario, your subscriber function may take input data from
+multiple tables, process it and create a derived output that is then sent to an external system.
+
+Register this subscriber function to the `CUSTOMERS` collection using the following command:
+
+```
+td fn register --collection CUSTOMERS --fn-path $TDX/subscriber.py::subscribe_customers
+```
+
+You can now verify that the function was registered successfully bu running the following command:
+
+```
+td fn list --collection CUSTOMERS
+```
+
+This should confirm that the `subscribe_customers` has been registered within the collection `CUSTOMERS`.
+
+### 3.2 Triggering the subscriber for the first time
+
+As is also the case with publisher functions, registering the subscriber function does not execute it. It must be
+executed by a trigger. In this step we will manually trigger the subscriber function for the first time and verify
+the generated output.
+
+We begin by making sure that there is no output directory present on our system. The following command should error
+out:
+
+```
+ls $TDX/output
+```
+
+If this directory exists, go ahead and delete it. When the subscribe function is triggerd it will create the necessary
+output directory store the output file.
 
-**tables** defines the name of the table (`CUSTOMER_LEADS`) to be read from the Tabsdata collection.
+Let's now manually trigger our subscriber function using the following command:
+
+```
+td fn trigger --collection CUSTOMERS --name subscribe_customers
+```
+
+As in the case of publisher functions, the triggering of a subscriber fucntion also generates an execution plan and
+sets it in motion. You can more about this in
+[Tabsdata documentation](https://docs.tabsdata.com/latest/guide/04_working_with_functions/main_1.html).
 
-**destination** parameter defines the path to the folder, the file name, and the file format, to be written by the subscriber.
 
-## Check the Subscriber Output:
+### 3.3 Checking the subscriber output:
 
-Once executed, the subscriber would have generated the output file `customer_leads.jsonl` in the `output` directory.
+Once executed, the subscriber would have generated the output file `customer_leads.jsonl` in the `$TDX/output`
+directory.
 
 Here is some sample data from `customer_leads.jsonl`:
 
@@ -197,39 +344,72 @@ Here is some sample data from `customer_leads.jsonl`:
 {"IDENTIFIER":"37-41/89","GENDER":"Male","NATIONALITY":"Cuban","LANGUAGE":"Luxembourgish","OCCUPATION":"Telex Operator"}
 ```
 
-As you see from the output file, only the columns selected from the `customers.csv` defined in `publisher.py` file have been exported, and the `jsonl` file is ready for consumption.
+As you see from the output file, only the columns selected from the `customers.csv` defined in `publisher.py` file have
+been exported, and the `jsonl` file is ready for consumption.
 
-# Step 4: Automate Data Engineering
+## Step 4: Automatic execution of dependencies
 
 What happens when there is an update in your input data? How do you update the data used by the downstream users?
 
-Let’s say there is an update in your CSV file, and 20 new customers get added to the CSV file. The `customers.csv` file in the `input_02` folder presents one such scenario. The file has 20 new customers in addition to the customers present in the `customers.csv` file in the `input` folder.
+Let’s say there is an update in your CSV file, and 20 new customers get added to the CSV file. The `customers_02.csv`
+file in the `input` directory presents one such scenario. This file has 20 new customers in addition to the customers
+present in the `customers_01.csv` file that we loaded via the publisher when we triggered it for the first time.
 
 Here are details of 3 of these new customers from the 20 who have been added:
 
 | IDENTIFIER | NAME       | SURNAME | FIRST_NAME | LAST_NAME | FULL_NAME       | GENDER | GENDER_CODE | GENDER_SYMBOL | SEX    | PHONE_NUMBER  | TELEPHONE      | EMAIL                          | BIRTHDATE  | NATIONALITY | LANGUAGE | LOCALE    | BLOOD_TYPE | HEIGHT | WEIGHT | UNIVERSITY                                         | ACADEMIC_DEGREE | TITLE | OCCUPATION           | POLITICAL_VIEWS | WORLDVIEW          | USERNAME          | PASSWORD     |
 |-------------|------------|---------|-------------|-----------|------------------|--------|--------------|----------------|--------|----------------|-----------------|-------------------------------|------------|--------------|-----------|------------|-------------|--------|--------|-----------------------------------------------------|------------------|-------|----------------------|------------------|---------------------|--------------------|--------------|
 | 21-12/62     | Dakota      | Baxter  | Louetta      | Myers     | Tracy Ball        | Other  | 1            | ♂               | Female | +12272974320   | +16146882188    | drainage2086@duck.com         | 2022-11-04 | Swiss        | Zulu      | Locale.EN  | A+          | 1.79   | 38     | Western Connecticut State University (WCSU)        | PhD               | Mr.   | Medical Technician     | Moderate          | Pantheism           | networks_1867       | Gvwp+R+N     |
-| 97-89/11     | Christinia  | Espinoza| Elden        | Alvarado  | Gerald Wolfe      | Female | 0            | ⚲               | Other  | +1-402-266-2114| +1-479-878-9781 | livestock1811@example.org     | 2014-05-09 | Dominican    | Yiddish   | Locale.EN  | A+          | 1.80   | 74     | Florida Gulf Coast University (FGCU)               | PhD               | Ms.   | Maid                  | Socialist          | Atheism             | throw_1882          | }&<h*EYp     |
+| 97-89/11     | Christinia  | Espinoza| Elden        | Alvarado  | Gerald Wolfe      | Female | 0            | ⚲               | Other  | +1-402-266-2114| +1-479-878-9781 | livestock1811@example.org     | 2014-05-09 | Dominican    | Yiddish   | Locale.EN  | A+          | 1.80   | 74     | Florida Gulf Coast University (FGCU)               | PhD               | Ms.   | Maid                  | Socialist          | Atheism             | throw_1882          | ?     |
 | 92-54/93     | Perry       | Herman  | Amina        | Montgomery| Lory Justice      | Other  | 0            | ♀               | Female | +1-817-696-6699| +1-213-091-1513 | dynamic2052@duck.com          | 2018-12-24 | Salvadorian  | Greek     | Locale.EN  | B+          | 1.66   | 42     | University of South Florida (USF)                   | Bachelor           | B.Sc  | Town Planner           | Libertarian        | Secular humanism    | excitement_1908      | ]X4&n9yn     |
 
 <br/>
 
-**Important:** To simulate the new customers data being available as input, you need to replace the `customers.csv` file in the `input` folder with the `customers_new.csv` file in `input_02` and rename the it to `customers.csv`. With this, the new `customers.csv` in the `input` folder would have the data of 20 new customers.
+### 4.1 Provisioning new input file
+
+Before we can demonstrate the automatic execution of this workflow, we must provision the new input file in the correct
+location for the publisher to read and publish it accordingly. This can be done using the following command:
+
+```
+cp $TDX/input/customers_02.csv $TDX/input/customers.csv
+```
+
+This will overwrite the `customers.csv` file that was previously copied from `customers_01.csv` file for our first
+execution.
 
-Once the new input file is available, you just need to execute the publisher `publish_customers` using the command below to update the data files used by the downstream users.
+### 4.2 Saving first output file for comparison
+
+When this new workflow executes, the subscriber will overwrite the output file `$TDX/output/customer_leads.jsonl` with
+new data. To ensure this, let's create a backup of this file for later comparison using the following command:
+
+```
+cp $TDX/output/customer_leads.jsonl $TDX/output/customer_leads_01.jsonl
+```
+
+### 4.3 Trigger the pub/sub workflow
+
+The publisher function that we registered earlier creates a table called `CUSTOMER_LEADS`. This table in turn has a
+registered subscriber. Together, this publisher/subscriber pair makes a simple data engineering workflow. When the
+publisher activates and updates the table, it will automatically trigger any subscribers for the updated tables.
+
+To demonstrate this, we will trigger our publisher function manually. This should automatically trigger the subscriber
+function which in turn should overwrite our expected output file. Since the new input file has 20 more customer
+records, we expect that the output file will also have 20 more customer records available.
+
+Use the following command to trigger the publisher to read new input file:
 
 ```
 td fn trigger --collection CUSTOMERS --name publish_customers
 ```
 
-Once the publisher executes, all the downstream functions that are dependent on the output table `CUSTOMER_LEADS` from `publish_customers` would get executed to generate their respective output data.
+Even though there is only one subscriber that was executed on refresh of the published table, it will work for any
+number of subscribers that are registered.
 
-In our current example, since `susbcribe_customers` is dependent on the `CUSTOMER_LEADS` table, the subscriber would get executed to generate a new version of `customer_leads.jsonl` file in the local file system. You can read more about the automated dependency management in the Tabsdata documentation.
 
-## Check the Output:
+### 4.4 Check the subscriber output
 
-Once the publisher has been executed, you can check the `customer_leads.jsonl` file in the `output` folder to see if the changes are getting reflected.
+Once the publisher has been executed, you can check the `customer_leads.jsonl` file in the `output` folder to see if
+the changes are getting reflected.
 
 Here is some sample data from the new `customer_leads.jsonl`:
 
@@ -239,24 +419,32 @@ Here is some sample data from the new `customer_leads.jsonl`:
 {"IDENTIFIER":"92-54/93","GENDER":"Other","NATIONALITY":"Salvadorian","LANGUAGE":"Greek","OCCUPATION":"Town Planner"}
 ```
 
-The above users were not present in the JSON file before, and have been added after the publisher was triggered with the new `customers.csv` file.
-
-This implies that after executing the publisher with the new input data, changes percolated downstream. In a real world environment, this would ensure that all the teams within an organization are looking at the same version of data.
-
-**Note:** If for any reason, the downstream team wishes to subscribe to the older version of the table, then can use simple Table commits syntax in Tabsdata to do that. You can read more about it in the Tabsdata documentation.
+The above users were not present in the JSON file before, and have been added after the publisher was triggered with
+the new `customers.csv` file. You can verify this by comparing the `customer_leads.jsonl` file with
+`customer_leads_01.jsonl` that we saved for comparison earlier.
 
 
-### Mission Accomplished!!
+## Conclusion
 
-We have successfully implemented a Pub/Sub for Tables using Tabsdata. We published the data from a CSV file as a table after selecting certain columns from it. We then subscribed to the resultant table. We also automated data engineering by automatically updating the output data when the input data changed.
+We have successfully implemented a Pub/Sub for Tables using Tabsdata. We published the data from a CSV file as a table
+after selecting certain columns from it. We then subscribed to the published table. We also demonstrated automatic
+execution of the entire workflow when a data source was refreshed.
 
 
-## Next Steps:
+## Next Steps
 
 For the next steps, here are a couple of experiements you can try:
 
-* Add a Tabsdata [transformer](https://docs.tabsdata.com/latest/guide/04_working_with_functions/working_with_transformers/main.html) in the mix. Perform complex transformations on ``CUSTOMER_LEADS`` table using a Tabsdata tranformer, and connect the output table from the transformer to a subscriber.
-* Read files from and write files to different external systems beyond local file system. You can read more about them [here](https://docs.tabsdata.com/latest/guide/supported_sources_and_destinations/main.html).
+* Add a Tabsdata
+  [transformer](https://docs.tabsdata.com/latest/guide/04_working_with_functions/working_with_transformers/main.html)
+  in the mix. Perform complex transformations on ``CUSTOMER_LEADS`` table using a Tabsdata tranformer, and connect the
+  output table from the transformer to a subscriber.
+* Read files from and write files to different external systems beyond local file system. You can read more about them
+  [here](https://docs.tabsdata.com/latest/guide/supported_sources_and_destinations/main.html).
 
 
-I hope this gave you a good introduction to the Tabsdata system! I'd love to hear your thoughts—let us know how we can improve, what use cases you'd like us to cover in future blogs, or any other questions or feedback you have. Join the conversation on [Discord](https://discord.gg/XRC5XZWppc), [Github Discussions](https://github.com/tabsdata/tabsdata/discussions) or reach out to us [here](https://www.tabsdata.com/contact).
\ No newline at end of file
+I hope this gave you a good introduction to the Tabsdata system! I'd love to hear your thoughts—let us know how we can
+improve, what use cases you'd like us to cover in future blogs, or any other questions or feedback you have. Join the
+conversation on [Discord](https://discord.gg/XRC5XZWppc),
+[Github Discussions](https://github.com/tabsdata/tabsdata/discussions) or reach out to us
+[here](https://www.tabsdata.com/contact).
diff --git a/t01_csv_pub_sub/input/customers.csv b/t01_csv_pub_sub/input/customers_01.csv
similarity index 100%
rename from t01_csv_pub_sub/input/customers.csv
rename to t01_csv_pub_sub/input/customers_01.csv
diff --git a/t01_csv_pub_sub/input_02/customers_new.csv b/t01_csv_pub_sub/input/customers_02.csv
similarity index 100%
rename from t01_csv_pub_sub/input_02/customers_new.csv
rename to t01_csv_pub_sub/input/customers_02.csv
diff --git a/t01_csv_pub_sub/subscriber.py b/t01_csv_pub_sub/subscriber.py
index eeacd64..73efcc5 100644
--- a/t01_csv_pub_sub/subscriber.py
+++ b/t01_csv_pub_sub/subscriber.py
@@ -6,8 +6,8 @@ import tabsdata as td
     tables = ["CUSTOMER_LEADS"],
 
     # Absolute system path to the file to be written by the Subscriber.
-    destination = td.LocalFileDestination(os.path.join(os.getenv("TDX"), "output", "CUSTOMER_LEADS.jsonl")),
+    destination = td.LocalFileDestination(os.path.join(os.getenv("TDX"), "output", "customer_leads.jsonl")),
 )
 
 def subscribe_customers(tf: td.TableFrame):
-    return tf
\ No newline at end of file
+    return tf
